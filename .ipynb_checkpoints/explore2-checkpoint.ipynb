{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "%matplotlib inline\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/cleaned_ca.csv')\n",
    "p_df_75 = pd.read_csv(io.StringIO(pd.read_csv('data/principal_component_df.csv').to_csv(index=False)), index_col=0)\n",
    "p_df_100 = pd.read_csv(io.StringIO(pd.read_csv('data/principal_component_df100.csv').to_csv(index=False)), index_col=0)\n",
    "X = pd.read_csv('data/X.csv')\n",
    "y = pd.read_csv('data/y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "# Train test split\n",
    "Xp75_train, Xp75_test, yp75_train, yp75_test = train_test_split(p_df_75, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "assert(len(Xp75_train) == len(yp75_train))\n",
    "assert(len(Xp75_test) == len(yp75_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8005, 75)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xp75_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "rnd_clf = RandomForestClassifier(n_estimators= 100,n_jobs=-1, random_state=27)\n",
    "rnd_clf.fit(Xp75_train, yp75_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rnd_clf.predict(Xp75_test)\n",
    "test_acc = np.sum(yp75_test_pred == yp75_test)/len(yp75_test)\n",
    "print (\"\\nAUC - ROC : \", roc_auc_score(yp75_test,rnd_clf.predict(Xp75_test)))\n",
    "print(\"test accuracy: \"+str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(rnd_clf)\n",
    "shap_values = explainer.shap_values(Xp75_test)\n",
    "shap.summary_plot(shap_values[1], Xp75_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_input\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(yp75_test[:,3], y_test_pred[:,3])\n",
    "resp_1 = confusion_matrix(yp75_test[:,0], y_test_pred[:,0])\n",
    "resp_2 = confusion_matrix(yp75_test[:,1], y_test_pred[:,1])\n",
    "resp_3 = confusion_matrix(yp75_test[:,2], y_test_pred[:,2])\n",
    "resp_4 = confusion_matrix(yp75_test[:,4], y_test_pred[:,4])\n",
    "all_resp = np.add(np.add(np.add(conf_mat, resp_1),np.add(resp_2, resp_3)),resp_4)\n",
    "sns.heatmap(all_resp, cmap=\"RdYlGn\", annot=True).set_title(\"All resp summed confusion matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "print(\"resp\")\n",
    "print(conf_mat)\n",
    "print(\"resp_1\")\n",
    "print(resp_1)\n",
    "print(\"resp_2\")\n",
    "print(resp_2)\n",
    "print(\"resp_3\")\n",
    "print(resp_3)\n",
    "print(\"resp_4\")\n",
    "print(resp_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(\n",
    "    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "):\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=(num_columns,))\n",
    "    x = tf.keras.layers.BatchNormalization()(inp)\n",
    "    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n",
    "    for i in range(len(hidden_units)):\n",
    "        x = tf.keras.layers.Dense(hidden_units[i])(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n",
    "    x = tf.keras.layers.Dense(num_labels)(x)\n",
    "    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n",
    "        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide_output\n",
    "batch_size = 124\n",
    "hidden_units = [150, 150, 150]\n",
    "dropout_rates = [0.20, 0.20, 0.20, 0.20]\n",
    "label_smoothing = 1e-2\n",
    "learning_rate = 3e-3\n",
    "\n",
    "#with tpu_strategy.scope():\n",
    "clf = create_mlp(\n",
    "        Xp75_train.shape[1], 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n",
    "    )\n",
    "\n",
    "clf.fit(Xp75_train, yp75_train, epochs=10, batch_size=batch_size)\n",
    "\n",
    "models.append(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
